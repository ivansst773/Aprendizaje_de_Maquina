{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Environments\nEstos entornos permiten 3 intentos (vidas) para superar las 32 etapas en el juego. Los entornos solo envían fotogramas de juego recompensables a agentes; No se envían escenas de corte, pantallas de carga, etc. desde el emulador de NES a un agente ni puede un agente realizar acciones durante estas instancias. Si un La escena no se puede saltar pirateando la RAM de la NES, el entorno bloqueará el proceso de Python hasta que el emulador esté listo para la siguiente acción.\n\n\n![image.png](attachment:a752f15f-60ce-41f6-9f1d-eda2824c5ad1.png)\n\n","metadata":{},"attachments":{"a752f15f-60ce-41f6-9f1d-eda2824c5ad1.png":{"image/png":"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"},"d51fbed4-91f2-4b7f-ab18-9391f4adfbc8.png":{"image/png":"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"}}},{"cell_type":"code","source":"#librerias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install swig\n!pip install gym[atari,box2d,accept-rom-license]  #install gym and virtual display\n!pip install gym-super-mario-bros","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:38:09.040508Z","iopub.execute_input":"2025-07-04T21:38:09.040933Z","iopub.status.idle":"2025-07-04T21:41:28.248243Z","shell.execute_reply.started":"2025-07-04T21:38:09.040906Z","shell.execute_reply":"2025-07-04T21:41:28.246583Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tf_agents.environments import tf_py_environment, gym_wrapper, tf_py_environment\nfrom tf_agents.environments.wrappers import ActionRepeat\nfrom tf_agents.networks.q_network import QNetwork\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\nfrom tf_agents.utils import common\nfrom tf_agents.replay_buffers import TFUniformReplayBuffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.policies import random_tf_policy\n\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')\n\nfrom IPython.display import HTML\n\nimport gym\nfrom nes_py.wrappers import JoypadSpace\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:13:29.823877Z","iopub.execute_input":"2025-07-04T22:13:29.824353Z","iopub.status.idle":"2025-07-04T22:13:29.831812Z","shell.execute_reply.started":"2025-07-04T22:13:29.824313Z","shell.execute_reply":"2025-07-04T22:13:29.830566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = gym_super_mario_bros.make('SuperMarioBros-v1')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nprint(env.get_action_meanings())\nstate = env.reset()\nenv.step(env._action_space.sample())\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:13:54.272775Z","iopub.execute_input":"2025-07-04T22:13:54.273173Z","iopub.status.idle":"2025-07-04T22:13:54.911332Z","shell.execute_reply.started":"2025-07-04T22:13:54.273149Z","shell.execute_reply":"2025-07-04T22:13:54.910291Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear el entorno\ndef create_environment():\n    import gym\n    from nes_py.wrappers import JoypadSpace\n    import gym_super_mario_bros\n    from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n\n\n# Crear una política aleatoria\nrandom_policy = random_tf_policy.RandomTFPolicy(\n    time_step_spec=eval_env.time_step_spec(),\n    action_spec=eval_env.action_spec()\n)\n\n# Definir la función para ejecutar y visualizar la política aleatoria\ndef run_and_visualize_random(policy, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    policy_state = policy.get_initial_state(env.batch_size)\n    step_count = 0\n    while not time_step.is_last() and step_count < max_steps:\n        action_step = policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n        # Captura la observación actual\n        frame = time_step.observation.numpy()[0]\n        # Transforma la observación para visualizar\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n        step_count += 1\n    return frames\n\n# Definir las funciones de animación\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar la política aleatoria y visualizar el juego\nframes = run_and_visualize_random(random_policy, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:14:27.785954Z","iopub.execute_input":"2025-07-04T22:14:27.786328Z","iopub.status.idle":"2025-07-04T22:15:09.095668Z","shell.execute_reply.started":"2025-07-04T22:14:27.786306Z","shell.execute_reply":"2025-07-04T22:15:09.094002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create and wrap the environment\ndef create_environment():\n    # Load the Super Mario Bros environment\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    # Simplify the action space\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    # Apply preprocessing wrappers\n    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n    env = gym.wrappers.ResizeObservation(env, 84)\n    env = gym.wrappers.FrameStack(env, 4)\n    env = gym_wrapper.GymWrapper(env)\n    return env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:18:25.985936Z","iopub.execute_input":"2025-07-04T22:18:25.98628Z","iopub.status.idle":"2025-07-04T22:18:25.99231Z","shell.execute_reply.started":"2025-07-04T22:18:25.98626Z","shell.execute_reply":"2025-07-04T22:18:25.991247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate training and evaluation environments\ntrain_py_env = create_environment()\neval_py_env = create_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:18:59.675268Z","iopub.execute_input":"2025-07-04T22:18:59.676476Z","iopub.status.idle":"2025-07-04T22:19:00.600284Z","shell.execute_reply.started":"2025-07-04T22:18:59.676442Z","shell.execute_reply":"2025-07-04T22:19:00.598642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to TensorFlow Agents environments\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:12.171337Z","iopub.execute_input":"2025-07-04T22:19:12.171634Z","iopub.status.idle":"2025-07-04T22:19:12.18609Z","shell.execute_reply.started":"2025-07-04T22:19:12.171614Z","shell.execute_reply":"2025-07-04T22:19:12.185014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the Q-network\npreprocessing_layer = tf.keras.layers.Lambda(lambda x: tf.cast(x, np.float32) / 255.)\nconv_layer_params = [\n    (32, (8, 8), 4),\n    (64, (4, 4), 2),\n    (64, (3, 3), 1),\n]\nfc_layer_params = [512]\n\nq_net = QNetwork(\n    input_tensor_spec=train_env.observation_spec(),\n    action_spec=train_env.action_spec(),\n    preprocessing_layers=preprocessing_layer,\n    conv_layer_params=conv_layer_params,\n    fc_layer_params=fc_layer_params\n)\n\n# Create the DQN agent\noptimizer = tf.compat.v1.train.RMSPropOptimizer(\n    learning_rate=2.5e-4,\n    decay=0.95,\n    momentum=0.0,\n    epsilon=0.01,\n    centered=True\n)\ntrain_step_counter = tf.Variable(0)\n\nagent = DqnAgent(\n    time_step_spec=train_env.time_step_spec(),\n    action_spec=train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter,\n    gamma=0.99,\n    epsilon_greedy=0.1,\n    target_update_period=10000\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:24.558494Z","iopub.execute_input":"2025-07-04T22:19:24.55893Z","iopub.status.idle":"2025-07-04T22:19:24.946414Z","shell.execute_reply.started":"2025-07-04T22:19:24.558906Z","shell.execute_reply":"2025-07-04T22:19:24.945302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.initialize()\n\n# Create the replay buffer\nreplay_buffer = TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=100000\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:31.964479Z","iopub.execute_input":"2025-07-04T22:19:31.964997Z","iopub.status.idle":"2025-07-04T22:19:34.894686Z","shell.execute_reply.started":"2025-07-04T22:19:31.964967Z","shell.execute_reply":"2025-07-04T22:19:34.893751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to collect experience\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n    buffer.add_batch(traj)\n\n# episode_rewards = []\n# current_episode_reward = 0\n\n# def collect_step(environment, policy, buffer, prev_x=[0]):\n#     global current_episode_reward, episode_rewards\n\n#     time_step = environment.current_time_step()\n#     action_step = policy.action(time_step)\n#     next_time_step = environment.step(action_step.action)\n\n#     # Extraer x_pos del entorno original\n#     # info = environment.pyenv.envs[0].get_info()\n#     # x_pos = info.get('x_pos', 0)\n    \n#     # Fallback al obtener x_pos para calcular la recompensa personalizada\n#     try:\n#         info = environment.pyenv.envs[0].get_info()\n#         x_pos = info.get('x_pos', 0)  # Si no está la clave, usa 0\n#     except Exception as e:\n#         print(f\"⚠️ Error al obtener x_pos: {e}\")\n#         x_pos = 0  # Fallback total\n    \n#     # Calcular recompensa basada en desplazamiento en x\n#     reward = x_pos - prev_x[0]\n#     prev_x[0] = x_pos\n\n#     # Acumular recompensa del episodio actual\n#     current_episode_reward += reward\n\n#     # Detectar si termina el episodio\n#     if next_time_step.is_last():\n#         print(f\"✅ Episodio terminado. Recompensa acumulada: {current_episode_reward}\")\n#         episode_rewards.append(current_episode_reward)\n#         current_episode_reward = 0\n\n#     # Reemplazar la recompensa\n#     next_time_step = next_time_step._replace(reward=tf.convert_to_tensor([reward], dtype=tf.float32))\n\n#     traj = trajectory.from_transition(time_step, action_step, next_time_step)\n#     buffer.add_batch(traj)\n    \n# Collect initial data with a random policy\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\ninitial_collect_steps = 1000\nfor _ in range(initial_collect_steps):\n    collect_step(train_env, random_policy, replay_buffer)\n\n# Prepare the dataset\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=32,\n    num_steps=2\n).prefetch(3)\niterator = iter(dataset)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:21:37.878183Z","iopub.execute_input":"2025-07-04T22:21:37.878604Z","iopub.status.idle":"2025-07-04T22:21:48.760189Z","shell.execute_reply.started":"2025-07-04T22:21:37.878576Z","shell.execute_reply":"2025-07-04T22:21:48.758983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training the agent\nnum_iterations = 100000  # Adjust this value based on Kaggle's computational limits\ncollect_steps_per_iteration = 1\nlog_interval = 300\n# Lista para almacenar las pérdidas\nlosses = []\n\nfor iteration in range(num_iterations):\n    # Collect experience\n    for _ in range(collect_steps_per_iteration):\n        collect_step(train_env, agent.collect_policy, replay_buffer)\n    # Sample a batch of data and train the agent\n    experience, _ = next(iterator)\n    train_loss = agent.train(experience).loss\n    if iteration % log_interval == 0:\n        print(f'Iteration: {iteration}, Loss: {train_loss}')\n        print(f'Iteración {iteration + 1}/{num_iterations}')\n        print(f'Pérdida de entrenamiento (loss): {train_loss.numpy():.4f}')\n        print(f'Pasos de entrenamiento acumulados: {train_step_counter.numpy()}')\n\n\n# try:\n#     for iteration in range(num_iterations):\n#         print(f\"\\n🔁 Iteración {iteration + 1}/{num_iterations}\")\n\n#         for _ in range(collect_steps_per_iteration):\n#             collect_step(train_env, agent.collect_policy, replay_buffer)\n\n#         experience, _ = next(iterator)\n#         train_loss = agent.train(experience).loss\n\n#         losses.append(train_loss.numpy())  # Guardar pérdida\n\n\n#         print(f\"   Pérdida: {train_loss.numpy():.4f}\")\n#         print(f\"   Paso de entrenamiento: {train_step_counter.numpy()}\")\n\n# except Exception as e:\n#     print(f\"🚨 Error durante el entrenamiento en la iteración {iteration}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:22:35.83196Z","iopub.execute_input":"2025-07-04T22:22:35.832334Z","iopub.status.idle":"2025-07-04T22:22:39.117048Z","shell.execute_reply.started":"2025-07-04T22:22:35.832309Z","shell.execute_reply":"2025-07-04T22:22:39.115955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Graficar la pérdida\n# plt.figure(figsize=(10, 6))\n# plt.plot(losses, label='Pérdida por iteración')\n# plt.xlabel('Iteración')\n# plt.ylabel('Pérdida')\n# plt.title('Evolución de la Pérdida durante el Entrenamiento')\n# plt.grid(True)\n# plt.legend()\n# plt.show()\n\n# plt.figure(figsize=(10, 6))\n# plt.plot(episode_rewards, label='Recompensa total por episodio')\n# plt.xlabel('Episodio')\n# plt.ylabel('Recompensa acumulada (x_pos)')\n# plt.title('Progreso del agente de Mario')\n# plt.grid(True)\n# plt.legend()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:29:23.155484Z","iopub.execute_input":"2025-07-04T22:29:23.155862Z","iopub.status.idle":"2025-07-04T22:29:23.161431Z","shell.execute_reply.started":"2025-07-04T22:29:23.155821Z","shell.execute_reply":"2025-07-04T22:29:23.160492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the agent's performance\nlog_interval = 5\nnum_eval_episodes = 2\nfor episode in range(num_eval_episodes):\n    time_step = eval_env.reset()\n    episode_reward = 0\n    i = 0\n    while not time_step.is_last():\n        action_step = agent.policy.action(time_step)\n        time_step = eval_env.step(action_step.action)\n        episode_reward += time_step.reward\n        i+=1\n       \n        if i % log_interval == 0:\n            print(f'Episode {episode + 1}: Ite.: {i} :episode_reward : {episode_reward.numpy()[0]}')\n    print(f'Episode {episode + 1}: Reward = {episode_reward.numpy()[0]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:29:31.75904Z","iopub.execute_input":"2025-07-04T22:29:31.75943Z","iopub.status.idle":"2025-07-04T22:49:48.626688Z","shell.execute_reply.started":"2025-07-04T22:29:31.759401Z","shell.execute_reply":"2025-07-04T22:49:48.625541Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom collections import deque\nfrom tf_agents.environments import suite_gym, tf_py_environment\nfrom tf_agents.environments import wrappers as tf_env_wrappers\nfrom tf_agents.environments import gym_wrapper\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nimport gym_super_mario_bros\n\n# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Preprocesamiento de observaciones\ndef preprocess_observation(obs):\n    obs = tf.image.rgb_to_grayscale(obs)\n    obs = tf.image.resize(obs, [84, 84])\n    obs = tf.cast(obs, tf.uint8)\n    return obs\n\n# Ejecutar y visualizar la política aleatoria\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    raw_obs = time_step.observation[0]\n    processed_obs = preprocess_observation(raw_obs)\n\n    # Inicializar pila de frames\n    frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Apilar frames\n        # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n        # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n        # # Crear nuevo time_step con observación apilada\n        # time_step = time_step._replace(observation=stacked_obs)\n        \n        # Apilar frames correctamente\n        stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n        stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n        # Expandir dimensión de batch\n        stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n        # Crear nuevo time_step con observación apilada\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acción\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Capturar frame para visualización\n        frame = time_step.observation.numpy()[0]\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n\n        # Actualizar pila de frames\n        new_obs = preprocess_observation(time_step.observation[0])\n        frame_stack.append(new_obs)\n\n        step_count += 1\n\n    return frames\n\n# Funciones de animación\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar y visualizar\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom collections import deque\nfrom tf_agents.environments import suite_gym, tf_py_environment\nfrom tf_agents.environments import wrappers as tf_env_wrappers\nfrom tf_agents.environments import gym_wrapper\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nimport gym_super_mario_bros\n\n# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Preprocesamiento de observaciones\ndef preprocess_observation(obs):\n    obs = tf.image.rgb_to_grayscale(obs)\n    obs = tf.image.resize(obs, [84, 84])\n    obs = tf.cast(obs, tf.uint8)\n    return obs\n\n# Ejecutar y visualizar la política aleatoria\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    raw_obs = time_step.observation[0]\n    processed_obs = preprocess_observation(raw_obs)\n\n    # Inicializar pila de frames\n    frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Apilar frames\n        # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n        # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n        # # Crear nuevo time_step con observación apilada\n        # time_step = time_step._replace(observation=stacked_obs)\n        \n        # Apilar frames correctamente\n        stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n        stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n        # Expandir dimensión de batch\n        stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n        # Crear nuevo time_step con observación apilada\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acción\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Capturar frame para visualización\n        frame = time_step.observation.numpy()[0]\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n\n        # Actualizar pila de frames\n        new_obs = preprocess_observation(time_step.observation[0])\n        frame_stack.append(new_obs)\n\n        step_count += 1\n\n    return frames\n\n# Funciones de animación\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar y visualizar\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T23:00:06.920769Z","iopub.execute_input":"2025-07-04T23:00:06.921152Z","iopub.status.idle":"2025-07-04T23:01:10.484911Z","shell.execute_reply.started":"2025-07-04T23:00:06.921127Z","shell.execute_reply":"2025-07-04T23:01:10.482954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# import matplotlib.pyplot as plt\n# import matplotlib.animation as animation\n# from collections import deque\n# from tf_agents.environments import suite_gym, tf_py_environment\n# from tf_agents.environments import wrappers as tf_env_wrappers\n# from tf_agents.environments import gym_wrapper\n# from nes_py.wrappers import JoypadSpace\n# from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n# import gym_super_mario_bros\n\n# # Crear el entorno\n# def create_environment():\n#     env = gym_super_mario_bros.make('SuperMarioBros-v1')\n#     env = JoypadSpace(env, SIMPLE_MOVEMENT)\n#     env = gym_wrapper.GymWrapper(env)\n#     return env\n\n# eval_py_env = create_environment()\n# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# # Preprocesamiento de observaciones\n# def preprocess_observation(obs):\n#     obs = tf.image.rgb_to_grayscale(obs)\n#     obs = tf.image.resize(obs, [84, 84])\n#     obs = tf.cast(obs, tf.uint8)\n#     return obs\n\n# # Ejecutar y visualizar la política aleatoria\n# def run_and_visualize_random(agent, env, max_steps=1000):\n#     frames = []\n#     time_step = env.reset()\n#     raw_obs = time_step.observation[0]\n#     processed_obs = preprocess_observation(raw_obs)\n\n#     # Inicializar pila de frames\n#     frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n#     policy_state = agent.policy.get_initial_state(env.batch_size)\n#     step_count = 0\n\n#     while not time_step.is_last() and step_count < max_steps:\n#         # Apilar frames\n#         # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n#         # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n#         # # Crear nuevo time_step con observación apilada\n#         # time_step = time_step._replace(observation=stacked_obs)\n        \n#         # Apilar frames correctamente\n#         stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n#         stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n#         # Expandir dimensión de batch\n#         stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n#         # Crear nuevo time_step con observación apilada\n#         time_step = time_step._replace(observation=stacked_obs)\n\n#         # Obtener acción\n#         action_step = agent.policy.action(time_step, policy_state)\n#         policy_state = action_step.state\n#         time_step = env.step(action_step.action)\n\n#         # Capturar frame para visualización\n#         frame = time_step.observation.numpy()[0]\n#         frame = frame.astype(np.uint8)\n#         frames.append(frame)\n\n#         # Actualizar pila de frames\n#         new_obs = preprocess_observation(time_step.observation[0])\n#         frame_stack.append(new_obs)\n\n#         step_count += 1\n\n#     return frames\n\n# # Funciones de animación\n# def update_scene(num, frames, patch):\n#     patch.set_data(frames[num])\n#     return patch\n\n# def plot_animation(frames, repeat=False, interval=40):\n#     fig = plt.figure()\n#     patch = plt.imshow(frames[0])\n#     plt.axis('off')\n#     anim = animation.FuncAnimation(\n#         fig, update_scene, fargs=(frames, patch),\n#         frames=len(frames), repeat=repeat, interval=interval)\n#     plt.close()\n#     return anim\n\n# # Ejecutar y visualizar\n# frames = run_and_visualize_random(agent, eval_env, max_steps=1000)\n# anim = plot_animation(frames)\n\n# from IPython.display import HTML\n# HTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T19:58:57.464285Z","iopub.execute_input":"2025-07-02T19:58:57.464778Z","iopub.status.idle":"2025-07-02T20:00:06.484573Z","shell.execute_reply.started":"2025-07-02T19:58:57.464717Z","shell.execute_reply":"2025-07-02T20:00:06.482578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n\n# # Definir la función para ejecutar y visualizar la política aleatoria\n# def run_and_visualize_random(agent, env, max_steps=1000):\n#     frames = []\n#     time_step = env.reset()\n#     policy_state = agent.policy.get_initial_state(env.batch_size)\n#     step_count = 0\n#     while not time_step.is_last() and step_count < max_steps:\n#         action_step = agent.policy.action(time_step, policy_state)\n#         policy_state = action_step.state\n#         time_step = env.step(action_step.action)\n#         # Captura la observación actual\n#         frame = time_step.observation.numpy()[0]\n#         # Transforma la observación para visualizar\n#         frame = frame.astype(np.uint8)\n#         frames.append(frame)\n#         step_count += 1\n#     return frames\nfrom collections import deque\n\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    obs = time_step.observation.numpy()[0]\n\n    # Preprocesar y apilar\n    processed = preprocess_observation(obs).numpy()  # (84, 84, 1)\n    frame_stack = deque([processed] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Stack y forma final: (1, 4, 84, 84, 1)\n        stacked_obs = np.stack(frame_stack, axis=0)\n        stacked_obs = np.expand_dims(stacked_obs, axis=0)\n\n        # Reemplazar observación en el TimeStep\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acción del agente\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Actualizar el stack de frames\n        new_obs = time_step.observation.numpy()[0]\n        processed = preprocess_observation(new_obs).numpy()\n        frame_stack.append(processed)\n\n        # Guardar frame original (sin procesar) para visualización\n        frames.append(new_obs.astype(np.uint8))\n\n        step_count += 1\n\n    return frames\n\n\n# Definir las funciones de animación\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar la política aleatoria y visualizar el juego\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T23:48:28.057671Z","iopub.execute_input":"2025-07-04T23:48:28.058541Z","iopub.status.idle":"2025-07-04T23:49:31.188372Z","shell.execute_reply.started":"2025-07-04T23:48:28.05851Z","shell.execute_reply":"2025-07-04T23:49:31.186804Z"}},"outputs":[],"execution_count":null}]}
