{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Environments\nEstos entornos permiten 3 intentos (vidas) para superar las 32 etapas en el juego. Los entornos solo env√≠an fotogramas de juego recompensables a agentes; No se env√≠an escenas de corte, pantallas de carga, etc. desde el emulador de NES a un agente ni puede un agente realizar acciones durante estas instancias. Si un La escena no se puede saltar pirateando la RAM de la NES, el entorno bloquear√° el proceso de Python hasta que el emulador est√© listo para la siguiente acci√≥n.\n\n\n![image.png](attachment:a752f15f-60ce-41f6-9f1d-eda2824c5ad1.png)\n\n","metadata":{},"attachments":{"a752f15f-60ce-41f6-9f1d-eda2824c5ad1.png":{"image/png":"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"},"d51fbed4-91f2-4b7f-ab18-9391f4adfbc8.png":{"image/png":"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"}}},{"cell_type":"code","source":"#librerias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install swig\n!pip install gym[atari,box2d,accept-rom-license]  #install gym and virtual display\n!pip install gym-super-mario-bros","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T21:38:09.040508Z","iopub.execute_input":"2025-07-04T21:38:09.040933Z","iopub.status.idle":"2025-07-04T21:41:28.248243Z","shell.execute_reply.started":"2025-07-04T21:38:09.040906Z","shell.execute_reply":"2025-07-04T21:41:28.246583Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom tf_agents.environments import tf_py_environment, gym_wrapper, tf_py_environment\nfrom tf_agents.environments.wrappers import ActionRepeat\nfrom tf_agents.networks.q_network import QNetwork\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\nfrom tf_agents.utils import common\nfrom tf_agents.replay_buffers import TFUniformReplayBuffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.policies import random_tf_policy\n\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')\n\nfrom IPython.display import HTML\n\nimport gym\nfrom nes_py.wrappers import JoypadSpace\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:13:29.823877Z","iopub.execute_input":"2025-07-04T22:13:29.824353Z","iopub.status.idle":"2025-07-04T22:13:29.831812Z","shell.execute_reply.started":"2025-07-04T22:13:29.824313Z","shell.execute_reply":"2025-07-04T22:13:29.830566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = gym_super_mario_bros.make('SuperMarioBros-v1')\nenv = JoypadSpace(env, SIMPLE_MOVEMENT)\nprint(env.get_action_meanings())\nstate = env.reset()\nenv.step(env._action_space.sample())\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:13:54.272775Z","iopub.execute_input":"2025-07-04T22:13:54.273173Z","iopub.status.idle":"2025-07-04T22:13:54.911332Z","shell.execute_reply.started":"2025-07-04T22:13:54.273149Z","shell.execute_reply":"2025-07-04T22:13:54.910291Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear el entorno\ndef create_environment():\n    import gym\n    from nes_py.wrappers import JoypadSpace\n    import gym_super_mario_bros\n    from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n\n\n# Crear una pol√≠tica aleatoria\nrandom_policy = random_tf_policy.RandomTFPolicy(\n    time_step_spec=eval_env.time_step_spec(),\n    action_spec=eval_env.action_spec()\n)\n\n# Definir la funci√≥n para ejecutar y visualizar la pol√≠tica aleatoria\ndef run_and_visualize_random(policy, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    policy_state = policy.get_initial_state(env.batch_size)\n    step_count = 0\n    while not time_step.is_last() and step_count < max_steps:\n        action_step = policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n        # Captura la observaci√≥n actual\n        frame = time_step.observation.numpy()[0]\n        # Transforma la observaci√≥n para visualizar\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n        step_count += 1\n    return frames\n\n# Definir las funciones de animaci√≥n\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar la pol√≠tica aleatoria y visualizar el juego\nframes = run_and_visualize_random(random_policy, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())\n","metadata":{"execution":{"iopub.status.busy":"2025-07-04T22:14:27.785954Z","iopub.execute_input":"2025-07-04T22:14:27.786328Z","iopub.status.idle":"2025-07-04T22:15:09.095668Z","shell.execute_reply.started":"2025-07-04T22:14:27.786306Z","shell.execute_reply":"2025-07-04T22:15:09.094002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create and wrap the environment\ndef create_environment():\n    # Load the Super Mario Bros environment\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    # Simplify the action space\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    # Apply preprocessing wrappers\n    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n    env = gym.wrappers.ResizeObservation(env, 84)\n    env = gym.wrappers.FrameStack(env, 4)\n    env = gym_wrapper.GymWrapper(env)\n    return env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:18:25.985936Z","iopub.execute_input":"2025-07-04T22:18:25.98628Z","iopub.status.idle":"2025-07-04T22:18:25.99231Z","shell.execute_reply.started":"2025-07-04T22:18:25.98626Z","shell.execute_reply":"2025-07-04T22:18:25.991247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate training and evaluation environments\ntrain_py_env = create_environment()\neval_py_env = create_environment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:18:59.675268Z","iopub.execute_input":"2025-07-04T22:18:59.676476Z","iopub.status.idle":"2025-07-04T22:19:00.600284Z","shell.execute_reply.started":"2025-07-04T22:18:59.676442Z","shell.execute_reply":"2025-07-04T22:19:00.598642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to TensorFlow Agents environments\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:12.171337Z","iopub.execute_input":"2025-07-04T22:19:12.171634Z","iopub.status.idle":"2025-07-04T22:19:12.18609Z","shell.execute_reply.started":"2025-07-04T22:19:12.171614Z","shell.execute_reply":"2025-07-04T22:19:12.185014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the Q-network\npreprocessing_layer = tf.keras.layers.Lambda(lambda x: tf.cast(x, np.float32) / 255.)\nconv_layer_params = [\n    (32, (8, 8), 4),\n    (64, (4, 4), 2),\n    (64, (3, 3), 1),\n]\nfc_layer_params = [512]\n\nq_net = QNetwork(\n    input_tensor_spec=train_env.observation_spec(),\n    action_spec=train_env.action_spec(),\n    preprocessing_layers=preprocessing_layer,\n    conv_layer_params=conv_layer_params,\n    fc_layer_params=fc_layer_params\n)\n\n# Create the DQN agent\noptimizer = tf.compat.v1.train.RMSPropOptimizer(\n    learning_rate=2.5e-4,\n    decay=0.95,\n    momentum=0.0,\n    epsilon=0.01,\n    centered=True\n)\ntrain_step_counter = tf.Variable(0)\n\nagent = DqnAgent(\n    time_step_spec=train_env.time_step_spec(),\n    action_spec=train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter,\n    gamma=0.99,\n    epsilon_greedy=0.1,\n    target_update_period=10000\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:24.558494Z","iopub.execute_input":"2025-07-04T22:19:24.55893Z","iopub.status.idle":"2025-07-04T22:19:24.946414Z","shell.execute_reply.started":"2025-07-04T22:19:24.558906Z","shell.execute_reply":"2025-07-04T22:19:24.945302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.initialize()\n\n# Create the replay buffer\nreplay_buffer = TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=100000\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:19:31.964479Z","iopub.execute_input":"2025-07-04T22:19:31.964997Z","iopub.status.idle":"2025-07-04T22:19:34.894686Z","shell.execute_reply.started":"2025-07-04T22:19:31.964967Z","shell.execute_reply":"2025-07-04T22:19:34.893751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to collect experience\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n    buffer.add_batch(traj)\n\n# episode_rewards = []\n# current_episode_reward = 0\n\n# def collect_step(environment, policy, buffer, prev_x=[0]):\n#     global current_episode_reward, episode_rewards\n\n#     time_step = environment.current_time_step()\n#     action_step = policy.action(time_step)\n#     next_time_step = environment.step(action_step.action)\n\n#     # Extraer x_pos del entorno original\n#     # info = environment.pyenv.envs[0].get_info()\n#     # x_pos = info.get('x_pos', 0)\n    \n#     # Fallback al obtener x_pos para calcular la recompensa personalizada\n#     try:\n#         info = environment.pyenv.envs[0].get_info()\n#         x_pos = info.get('x_pos', 0)  # Si no est√° la clave, usa 0\n#     except Exception as e:\n#         print(f\"‚ö†Ô∏è Error al obtener x_pos: {e}\")\n#         x_pos = 0  # Fallback total\n    \n#     # Calcular recompensa basada en desplazamiento en x\n#     reward = x_pos - prev_x[0]\n#     prev_x[0] = x_pos\n\n#     # Acumular recompensa del episodio actual\n#     current_episode_reward += reward\n\n#     # Detectar si termina el episodio\n#     if next_time_step.is_last():\n#         print(f\"‚úÖ Episodio terminado. Recompensa acumulada: {current_episode_reward}\")\n#         episode_rewards.append(current_episode_reward)\n#         current_episode_reward = 0\n\n#     # Reemplazar la recompensa\n#     next_time_step = next_time_step._replace(reward=tf.convert_to_tensor([reward], dtype=tf.float32))\n\n#     traj = trajectory.from_transition(time_step, action_step, next_time_step)\n#     buffer.add_batch(traj)\n    \n# Collect initial data with a random policy\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\ninitial_collect_steps = 1000\nfor _ in range(initial_collect_steps):\n    collect_step(train_env, random_policy, replay_buffer)\n\n# Prepare the dataset\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=32,\n    num_steps=2\n).prefetch(3)\niterator = iter(dataset)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:21:37.878183Z","iopub.execute_input":"2025-07-04T22:21:37.878604Z","iopub.status.idle":"2025-07-04T22:21:48.760189Z","shell.execute_reply.started":"2025-07-04T22:21:37.878576Z","shell.execute_reply":"2025-07-04T22:21:48.758983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training the agent\nnum_iterations = 100000  # Adjust this value based on Kaggle's computational limits\ncollect_steps_per_iteration = 1\nlog_interval = 300\n# Lista para almacenar las p√©rdidas\nlosses = []\n\nfor iteration in range(num_iterations):\n    # Collect experience\n    for _ in range(collect_steps_per_iteration):\n        collect_step(train_env, agent.collect_policy, replay_buffer)\n    # Sample a batch of data and train the agent\n    experience, _ = next(iterator)\n    train_loss = agent.train(experience).loss\n    if iteration % log_interval == 0:\n        print(f'Iteration: {iteration}, Loss: {train_loss}')\n        print(f'Iteraci√≥n {iteration + 1}/{num_iterations}')\n        print(f'P√©rdida de entrenamiento (loss): {train_loss.numpy():.4f}')\n        print(f'Pasos de entrenamiento acumulados: {train_step_counter.numpy()}')\n\n\n# try:\n#     for iteration in range(num_iterations):\n#         print(f\"\\nüîÅ Iteraci√≥n {iteration + 1}/{num_iterations}\")\n\n#         for _ in range(collect_steps_per_iteration):\n#             collect_step(train_env, agent.collect_policy, replay_buffer)\n\n#         experience, _ = next(iterator)\n#         train_loss = agent.train(experience).loss\n\n#         losses.append(train_loss.numpy())  # Guardar p√©rdida\n\n\n#         print(f\"   P√©rdida: {train_loss.numpy():.4f}\")\n#         print(f\"   Paso de entrenamiento: {train_step_counter.numpy()}\")\n\n# except Exception as e:\n#     print(f\"üö® Error durante el entrenamiento en la iteraci√≥n {iteration}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:22:35.83196Z","iopub.execute_input":"2025-07-04T22:22:35.832334Z","iopub.status.idle":"2025-07-04T22:22:39.117048Z","shell.execute_reply.started":"2025-07-04T22:22:35.832309Z","shell.execute_reply":"2025-07-04T22:22:39.115955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Graficar la p√©rdida\n# plt.figure(figsize=(10, 6))\n# plt.plot(losses, label='P√©rdida por iteraci√≥n')\n# plt.xlabel('Iteraci√≥n')\n# plt.ylabel('P√©rdida')\n# plt.title('Evoluci√≥n de la P√©rdida durante el Entrenamiento')\n# plt.grid(True)\n# plt.legend()\n# plt.show()\n\n# plt.figure(figsize=(10, 6))\n# plt.plot(episode_rewards, label='Recompensa total por episodio')\n# plt.xlabel('Episodio')\n# plt.ylabel('Recompensa acumulada (x_pos)')\n# plt.title('Progreso del agente de Mario')\n# plt.grid(True)\n# plt.legend()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:29:23.155484Z","iopub.execute_input":"2025-07-04T22:29:23.155862Z","iopub.status.idle":"2025-07-04T22:29:23.161431Z","shell.execute_reply.started":"2025-07-04T22:29:23.155821Z","shell.execute_reply":"2025-07-04T22:29:23.160492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the agent's performance\nlog_interval = 5\nnum_eval_episodes = 2\nfor episode in range(num_eval_episodes):\n    time_step = eval_env.reset()\n    episode_reward = 0\n    i = 0\n    while not time_step.is_last():\n        action_step = agent.policy.action(time_step)\n        time_step = eval_env.step(action_step.action)\n        episode_reward += time_step.reward\n        i+=1\n       \n        if i % log_interval == 0:\n            print(f'Episode {episode + 1}: Ite.: {i} :episode_reward : {episode_reward.numpy()[0]}')\n    print(f'Episode {episode + 1}: Reward = {episode_reward.numpy()[0]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T22:29:31.75904Z","iopub.execute_input":"2025-07-04T22:29:31.75943Z","iopub.status.idle":"2025-07-04T22:49:48.626688Z","shell.execute_reply.started":"2025-07-04T22:29:31.759401Z","shell.execute_reply":"2025-07-04T22:49:48.625541Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom collections import deque\nfrom tf_agents.environments import suite_gym, tf_py_environment\nfrom tf_agents.environments import wrappers as tf_env_wrappers\nfrom tf_agents.environments import gym_wrapper\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nimport gym_super_mario_bros\n\n# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Preprocesamiento de observaciones\ndef preprocess_observation(obs):\n    obs = tf.image.rgb_to_grayscale(obs)\n    obs = tf.image.resize(obs, [84, 84])\n    obs = tf.cast(obs, tf.uint8)\n    return obs\n\n# Ejecutar y visualizar la pol√≠tica aleatoria\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    raw_obs = time_step.observation[0]\n    processed_obs = preprocess_observation(raw_obs)\n\n    # Inicializar pila de frames\n    frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Apilar frames\n        # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n        # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n        # # Crear nuevo time_step con observaci√≥n apilada\n        # time_step = time_step._replace(observation=stacked_obs)\n        \n        # Apilar frames correctamente\n        stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n        stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n        # Expandir dimensi√≥n de batch\n        stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n        # Crear nuevo time_step con observaci√≥n apilada\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acci√≥n\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Capturar frame para visualizaci√≥n\n        frame = time_step.observation.numpy()[0]\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n\n        # Actualizar pila de frames\n        new_obs = preprocess_observation(time_step.observation[0])\n        frame_stack.append(new_obs)\n\n        step_count += 1\n\n    return frames\n\n# Funciones de animaci√≥n\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar y visualizar\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom collections import deque\nfrom tf_agents.environments import suite_gym, tf_py_environment\nfrom tf_agents.environments import wrappers as tf_env_wrappers\nfrom tf_agents.environments import gym_wrapper\nfrom nes_py.wrappers import JoypadSpace\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nimport gym_super_mario_bros\n\n# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Preprocesamiento de observaciones\ndef preprocess_observation(obs):\n    obs = tf.image.rgb_to_grayscale(obs)\n    obs = tf.image.resize(obs, [84, 84])\n    obs = tf.cast(obs, tf.uint8)\n    return obs\n\n# Ejecutar y visualizar la pol√≠tica aleatoria\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    raw_obs = time_step.observation[0]\n    processed_obs = preprocess_observation(raw_obs)\n\n    # Inicializar pila de frames\n    frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Apilar frames\n        # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n        # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n        # # Crear nuevo time_step con observaci√≥n apilada\n        # time_step = time_step._replace(observation=stacked_obs)\n        \n        # Apilar frames correctamente\n        stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n        stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n        # Expandir dimensi√≥n de batch\n        stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n        # Crear nuevo time_step con observaci√≥n apilada\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acci√≥n\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Capturar frame para visualizaci√≥n\n        frame = time_step.observation.numpy()[0]\n        frame = frame.astype(np.uint8)\n        frames.append(frame)\n\n        # Actualizar pila de frames\n        new_obs = preprocess_observation(time_step.observation[0])\n        frame_stack.append(new_obs)\n\n        step_count += 1\n\n    return frames\n\n# Funciones de animaci√≥n\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar y visualizar\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T23:00:06.920769Z","iopub.execute_input":"2025-07-04T23:00:06.921152Z","iopub.status.idle":"2025-07-04T23:01:10.484911Z","shell.execute_reply.started":"2025-07-04T23:00:06.921127Z","shell.execute_reply":"2025-07-04T23:01:10.482954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# import matplotlib.pyplot as plt\n# import matplotlib.animation as animation\n# from collections import deque\n# from tf_agents.environments import suite_gym, tf_py_environment\n# from tf_agents.environments import wrappers as tf_env_wrappers\n# from tf_agents.environments import gym_wrapper\n# from nes_py.wrappers import JoypadSpace\n# from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n# import gym_super_mario_bros\n\n# # Crear el entorno\n# def create_environment():\n#     env = gym_super_mario_bros.make('SuperMarioBros-v1')\n#     env = JoypadSpace(env, SIMPLE_MOVEMENT)\n#     env = gym_wrapper.GymWrapper(env)\n#     return env\n\n# eval_py_env = create_environment()\n# eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# # Preprocesamiento de observaciones\n# def preprocess_observation(obs):\n#     obs = tf.image.rgb_to_grayscale(obs)\n#     obs = tf.image.resize(obs, [84, 84])\n#     obs = tf.cast(obs, tf.uint8)\n#     return obs\n\n# # Ejecutar y visualizar la pol√≠tica aleatoria\n# def run_and_visualize_random(agent, env, max_steps=1000):\n#     frames = []\n#     time_step = env.reset()\n#     raw_obs = time_step.observation[0]\n#     processed_obs = preprocess_observation(raw_obs)\n\n#     # Inicializar pila de frames\n#     frame_stack = deque([processed_obs] * 4, maxlen=4)\n\n#     policy_state = agent.policy.get_initial_state(env.batch_size)\n#     step_count = 0\n\n#     while not time_step.is_last() and step_count < max_steps:\n#         # Apilar frames\n#         # stacked_obs = tf.concat(list(frame_stack), axis=-1)  # (84, 84, 4)\n#         # stacked_obs = tf.expand_dims(stacked_obs, axis=0)    # (1, 84, 84, 4)\n\n#         # # Crear nuevo time_step con observaci√≥n apilada\n#         # time_step = time_step._replace(observation=stacked_obs)\n        \n#         # Apilar frames correctamente\n#         stacked_obs = tf.stack(list(frame_stack), axis=0)  # (4, 84, 84, 1)\n#         stacked_obs = tf.cast(stacked_obs, tf.uint8)\n        \n#         # Expandir dimensi√≥n de batch\n#         stacked_obs = tf.expand_dims(stacked_obs, axis=0)  # (1, 4, 84, 84, 1)\n        \n#         # Crear nuevo time_step con observaci√≥n apilada\n#         time_step = time_step._replace(observation=stacked_obs)\n\n#         # Obtener acci√≥n\n#         action_step = agent.policy.action(time_step, policy_state)\n#         policy_state = action_step.state\n#         time_step = env.step(action_step.action)\n\n#         # Capturar frame para visualizaci√≥n\n#         frame = time_step.observation.numpy()[0]\n#         frame = frame.astype(np.uint8)\n#         frames.append(frame)\n\n#         # Actualizar pila de frames\n#         new_obs = preprocess_observation(time_step.observation[0])\n#         frame_stack.append(new_obs)\n\n#         step_count += 1\n\n#     return frames\n\n# # Funciones de animaci√≥n\n# def update_scene(num, frames, patch):\n#     patch.set_data(frames[num])\n#     return patch\n\n# def plot_animation(frames, repeat=False, interval=40):\n#     fig = plt.figure()\n#     patch = plt.imshow(frames[0])\n#     plt.axis('off')\n#     anim = animation.FuncAnimation(\n#         fig, update_scene, fargs=(frames, patch),\n#         frames=len(frames), repeat=repeat, interval=interval)\n#     plt.close()\n#     return anim\n\n# # Ejecutar y visualizar\n# frames = run_and_visualize_random(agent, eval_env, max_steps=1000)\n# anim = plot_animation(frames)\n\n# from IPython.display import HTML\n# HTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T19:58:57.464285Z","iopub.execute_input":"2025-07-02T19:58:57.464778Z","iopub.status.idle":"2025-07-02T20:00:06.484573Z","shell.execute_reply.started":"2025-07-02T19:58:57.464717Z","shell.execute_reply":"2025-07-02T20:00:06.482578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Crear el entorno\ndef create_environment():\n    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n    env = gym_wrapper.GymWrapper(env)\n    return env\n\neval_py_env = create_environment()\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n\n# # Definir la funci√≥n para ejecutar y visualizar la pol√≠tica aleatoria\n# def run_and_visualize_random(agent, env, max_steps=1000):\n#     frames = []\n#     time_step = env.reset()\n#     policy_state = agent.policy.get_initial_state(env.batch_size)\n#     step_count = 0\n#     while not time_step.is_last() and step_count < max_steps:\n#         action_step = agent.policy.action(time_step, policy_state)\n#         policy_state = action_step.state\n#         time_step = env.step(action_step.action)\n#         # Captura la observaci√≥n actual\n#         frame = time_step.observation.numpy()[0]\n#         # Transforma la observaci√≥n para visualizar\n#         frame = frame.astype(np.uint8)\n#         frames.append(frame)\n#         step_count += 1\n#     return frames\nfrom collections import deque\n\ndef run_and_visualize_random(agent, env, max_steps=1000):\n    frames = []\n    time_step = env.reset()\n    obs = time_step.observation.numpy()[0]\n\n    # Preprocesar y apilar\n    processed = preprocess_observation(obs).numpy()  # (84, 84, 1)\n    frame_stack = deque([processed] * 4, maxlen=4)\n\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    step_count = 0\n\n    while not time_step.is_last() and step_count < max_steps:\n        # Stack y forma final: (1, 4, 84, 84, 1)\n        stacked_obs = np.stack(frame_stack, axis=0)\n        stacked_obs = np.expand_dims(stacked_obs, axis=0)\n\n        # Reemplazar observaci√≥n en el TimeStep\n        time_step = time_step._replace(observation=stacked_obs)\n\n        # Obtener acci√≥n del agente\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n\n        # Actualizar el stack de frames\n        new_obs = time_step.observation.numpy()[0]\n        processed = preprocess_observation(new_obs).numpy()\n        frame_stack.append(processed)\n\n        # Guardar frame original (sin procesar) para visualizaci√≥n\n        frames.append(new_obs.astype(np.uint8))\n\n        step_count += 1\n\n    return frames\n\n\n# Definir las funciones de animaci√≥n\ndef update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim\n\n# Ejecutar la pol√≠tica aleatoria y visualizar el juego\nframes = run_and_visualize_random(agent, eval_env, max_steps=1000)\nanim = plot_animation(frames)\n\nfrom IPython.display import HTML\nHTML(anim.to_jshtml())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T23:48:28.057671Z","iopub.execute_input":"2025-07-04T23:48:28.058541Z","iopub.status.idle":"2025-07-04T23:49:31.188372Z","shell.execute_reply.started":"2025-07-04T23:48:28.05851Z","shell.execute_reply":"2025-07-04T23:49:31.186804Z"}},"outputs":[],"execution_count":null}]}
